{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20829,"status":"ok","timestamp":1659721348924,"user":{"displayName":"James Chen","userId":"10516375287081639459"},"user_tz":420},"id":"oamAqkVBwRkC","outputId":"2868e7d4-554e-4da2-92a4-202dc9b47e8e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from tqdm import tqdm\n","from datetime import datetime\n","import csv\n","import pandas as pd\n","import numpy as np\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BuR4Aua-hxmB"},"outputs":[],"source":["import pickle\n","path = \"drive/MyDrive/Knowledge Graphs 2022/data/\"\n","\n","with open(path+'big_dict.pickle', 'rb') as file:\n","    big_dict = pickle.load(file)\n","\n","# path2 = \"drive/MyDrive/Knowledge Graphs 2022/filtered users/dicts/\"\n","\n","# with open(path2+'filtered_big_dict_0.pickle', 'rb') as file:\n","#     filtered_big_dict_0 = pickle.load(file)\n","\n","# with open(path2+'filtered_big_dict_1.pickle', 'rb') as file:\n","#     filtered_big_dict_1 = pickle.load(file)\n","\n","# with open(path2+'filtered_big_dict_2.pickle', 'rb') as file:\n","#     filtered_big_dict_2 = pickle.load(file)\n","\n","path3 = \"drive/MyDrive/Knowledge Graphs 2022/bot detection/\"\n","\n","with open(path3+'systematic_bot_users.list', 'r') as file:\n","    bot_list = [int(x) for x in file.read().split('\\n')[:-1]]\n","\n","path4 = \"drive/MyDrive/Knowledge Graphs 2022/semantic network/\"\n","\n","with open(path4+'entities_w_labels.pickle', 'rb') as file:\n","    sem_ent_dict = pickle.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uFH1vLR6UqjN"},"outputs":[],"source":["# import pickle\n","# path = \"drive/MyDrive/Knowledge Graphs 2022/KG construction/\"\n","\n","# with open(path+'all_entities_filtered.pickle', 'rb') as file:\n","#     all_entities = pickle.load(file)\n","\n","# with open(path+'all_relations_filtered.pickle', 'rb') as file:\n","#     all_relations = pickle.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dVEedh_eVPON"},"outputs":[],"source":["# print(sum([len(all_entities[key]) for key in all_entities]))\n","# print(sum([len(all_relations[key]) for key in all_relations]))"]},{"cell_type":"markdown","metadata":{"id":"U4Ju7LnqeSXs"},"source":["## Panda"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YaX9jSK-IMs3"},"outputs":[],"source":["# 4 = user\n","# 21 = quoted_status\n","\n","pd.set_option('precision', 30)\n","tweet_df = pd.DataFrame.from_dict(big_dict)\n","tweet_df = tweet_df.where(pd.notnull(tweet_df), None) # null pre-processing, just in case\n","tweet_df = tweet_df.replace({np.nan: None}) # null pre-processing, just in case\n","tweet_df = tweet_df[~tweet_df['id'].duplicated()] # found repeats in the tweet data\n","mask = [user not in bot_list for user in [user['id'] for user in tweet_df['user']]] # filter bots\n","tweet_df = tweet_df[mask]\n","tweet_df['created_at'] = tweet_df['created_at'].apply(datetime.strptime, args = ('%a %b %d %H:%M:%S %z %Y',))\n","tweet_dict = tweet_df.to_dict(orient = 'records')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tz0M-RWdEwZj"},"outputs":[],"source":["topic_tweets = pd.read_csv(path+'tweets_w_recovered_topics_new.csv')\n","topic_tweets_dict = topic_tweets.to_dict(orient = 'records')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LUofz6v_OroK"},"outputs":[],"source":["# topic_set = set()\n","# for tweet in topic_tweets_dict:\n","#   topic_set.add(tweet['topic'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GJWkmp83PLUx"},"outputs":[],"source":["# topic_set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"doGYNKIz5dhD"},"outputs":[],"source":["emotion_tweets = pd.read_csv(path+'tweets_w_all_info.csv')\n","emotion_tweets_dict = emotion_tweets.to_dict(orient = 'records')"]},{"cell_type":"markdown","metadata":{"id":"VEqkYi1Me5fA"},"source":["## Create Indices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o3ysb61w1lre"},"outputs":[],"source":["# Event nodes\n","\n","event_dict_1 = {'October 28, 2017' : 'Start of: 4chan user “Q Clearance Patriot” posts conspiracies',\\\n","\n","'December 4, 2016': 'Comet Ping Pong',\\\n","\n","'December 21, 2017': 'US treasury dept global human rights abuses and corruption press release',\\\n","\n","'January 01, 2018': 'Qanon moves to 8chan',\\\n","\n","'January 02, 2018': 'Where we go one, we go all',\\\n","\n","'April 03, 2018': 'Q first uses WWG1WGA',\\\n","\n","'June 16, 2018': 'Q “hit list” of 101 reporters',\\\n","\n","'July 06, 2018': 'Arrest of Jeffery Epstein',\\\n","\n","'September 01, 2018': 'End of: Q rallies to vote Republican',\\\n","\n","'September 02, 2018': 'Start of: Midterm election fraud conspiracies',\\\n","\n","'September 04, 2018': 'Start of: Brett Kavanaugh confirmation hearing',\\\n","\n","'September 06, 2018': 'End of: Brett Kavanaugh confirmation hearing',\\\n","\n","'March 04, 2019': 'Trump claims Obama wiretapped him',\\\n","\n","'April 19, 2019': 'Muller report',\\\n","\n","'August 08, 2019': 'Cloudflare shuts down 8chan',\\\n","\n","'November 05, 2019': '8chan relaunches as 8kun',\\\n","\n","'August 19, 2020': 'Trump acknowledges Qanon',\\\n","\n","'November 03, 2020': 'Election of Joe Biden',\\\n","\n","'January 06, 2021': 'Capitol Riots'}\n","\n","date_to_event_dict = dict()\n","\n","for key,val in event_dict_1.items():\n","\n","  date_to_event_dict[datetime.strptime(key, '%B %d, %Y')] = val\n","\n","event_dict_2 = {'November 2016': 'Pizzagate',\\\n","                \n","'November 2017': 'End of: 4chan user “Q Clearance Patriot” posts conspiracies',\\\n","\n","'February 2018': 'Q drops tech conspiracies',\\\n","\n","'August 2018': 'Start of: Q rallies to vote Republican',\\\n","\n","'December 2018': 'End of: Midterm election fraud conspiracies'}\n","\n","for key,val in event_dict_2.items():\n","  \n","  date_to_event_dict[datetime.strptime(key, '%B %Y')] = val\n","\n","# Claiming amongst many other things that:\n","# A few major families (Clinton, Obama, Soros, Rothschild come up particularly often) controlled corrupt parts of US govt and work with a powerful global cabal to maintain control \n","# Predicted an upcoming storm of high profile arrests (which never happened) but these predictions gained a lot of traction\n","# This is a brief summary based on secondary sources, it may end up being useful to try and find the primary sources. \n","# This paper seems to have complied data from both qanon information aggregation sites and primary source “leaks” https://arxiv.org/abs/2101.08750\n","# https://nymag.com/intelligencer/2017/12/qanon-4chan-the-storm-conspiracy-explained.html\n","# https://i.imgur.com/1AqeIhH.png\n","# https://imgur.com/a/DTeK7\n","# https://archive.4plebs.org/pol/search/uid/BQ7V3bcW%20/order/asc/\n","# https://archive.4plebs.org/pol/thread/147146601/#147166292\n","# https://i.4pcdn.org/pol/1510107905656.jpg\n","# https://i.4pcdn.org/pol/1510107905656.jpg\n","\n","# stopped releasing new information, but supporters have continued spreading similar messages particular in particular the movement continues in 3 forms\n","# “Qanon proper” including individuals who focus on the original Q talking points about a global cabal involving Clintons Obamas etc\n","# “Stop the Steal” conspiracies which built of qanon \n","# “Pastel Qanon” with a larger focus on the child abuse aspect of the original conspiracy. Often adopted by wellness/spirituality/new age influencers and feature a “soft” aesthetic \n","# https://www.ft.com/content/d4921e71-c841-433e-a4a9-f85ff7ca263c\n"]},{"cell_type":"markdown","metadata":{"id":"M-JHoMfkk0F3"},"source":["## From here on out, much code inspired/taken from last year's REU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yjN7gT7We5NT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9ac4d953-acec-4bbf-fded-57a979ecfaeb"},"outputs":[{"output_type":"stream","name":"stderr","text":[" 99%|█████████▉| 266890/269376 [03:10<00:02, 875.21it/s]"]}],"source":["# the following entitiy set construction is taken from last year's REU\n","\n","all_entities = dict()\n","all_entities['tweet_ids'] = list(set(map(int, big_dict['id']))) \n","all_entities['geo'] = list(set([tuple(item['coordinates']) for item in big_dict['geo'] if item != None]))\n","all_entities['emotions'] = ['anger', 'joy', 'fear', 'sadness', 'surprise', 'love']\n","all_entities['events'] = list(set(date_to_event_dict.values()))\n","\n","dates = set()\n","tags = set()\n","users = set()\n","men_users = set()\n","sem_ents = set()\n","\n","for tweet in tweet_dict:\n","  \n","  dates.add(tweet['created_at'])\n","\n","  for tag in tweet['hashtags']:\n","    tags.add(tag['text'])\n","  \n","  users.add(tweet['user']['id'])\n","  \n","  for men in tweet['user_mentions']:\n","    men_users.add(men['id'])\n","\n","for tweet in tqdm(sem_ent_dict):\n","  sem_ents = sem_ents.union(set(sem_ent_dict[tweet]))\n","\n","for datetime in date_to_event_dict.keys():\n","  dates.add(datetime)\n","\n","print(len(men_users))\n","print(len(users))\n","\n","all_entities['dates'] = list(dates)\n","all_entities['hashtags'] = list(tags)\n","all_entities['user_ids'] = list(users.union(men_users))\n","all_entities['sem_ents'] = list(sem_ents)\n","all_entities['topics'] = list(range(50)) # naturally enumerated\n","\n","print(sum([len(all_entities[key]) for key in all_entities]))"]},{"cell_type":"code","source":["print(len(all_entities['hashtags']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dcmc8qCaQ4t5","executionInfo":{"status":"ok","timestamp":1659724086446,"user_tz":420,"elapsed":141,"user":{"displayName":"James Chen","userId":"10516375287081639459"}},"outputId":"57ee63bb-c768-41a4-d1cf-bfdd105836dc"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["66167\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDYhpej3cR2G"},"outputs":[],"source":["# the following is taken from last year's REU\n","\n","entity2ind = {i:{k:v for v,k in enumerate(all_entities[i])} for i in all_entities}\n","ind2entity = {i:{v:k for v,k in enumerate(all_entities[i])} for i in all_entities}"]},{"cell_type":"markdown","metadata":{"id":"IsafgqBmko3D"},"source":["## Relation Indices"]},{"cell_type":"markdown","metadata":{"id":"WOPem8lfklTy"},"source":["Relations: \n","\n","\n","*   replied_to: tweet -> tweet\n","*   tweeted: user -> tweet\n","*   has_hashtag: tweet -> hashtag\n","*   mentioned: tweet -> user\n","*   created_on: tweet -> date\n","*   announced_on: event -> date\n","*   has_keyword: tweet -> keyword *\n","*   associated_with: keyword -> topic *\n","*   in_topic: tweet -> topic *\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rGDm2fMlvA2p"},"source":["['retweet_count', 'display_text_range', 'in_reply_to_status_id_str',\n","       'withheld_in_countries', 'in_reply_to_screen_name', 'coordinates',\n","       'extended_entities', 'id', 'retweeted', 'user', 'quoted_status',\n","       'favorited', 'quoted_status_id_str', 'in_reply_to_user_id_str', 'place',\n","       'in_reply_to_user_id', 'source', 'is_quote_status',\n","       'possibly_sensitive', 'created_at', 'geo', 'in_reply_to_status_id',\n","       'id_str', 'full_text', 'favorite_count', 'entities', 'contributors',\n","       'lang', 'truncated', 'quoted_status_permalink', 'quoted_status_id',\n","       'urls', 'media', 'hashtags', 'symbols', 'user_mentions']\n","\n","['created_at',\n"," 'entities',\n"," 'author_id',\n"," 'lang',\n"," 'result_count',\n"," 'geo',\n"," 'id',\n"," 'context_annotations',\n"," 'in_reply_to_user_id',\n"," 'oldest_id',\n"," 'source',\n"," 'withheld',\n"," 'newest_id',\n"," 'text',\n"," 'reply_settings',\n"," 'attachments',\n"," 'possibly_sensitive',\n"," 'referenced_tweets',\n"," 'public_metrics',\n"," 'conversation_id']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RPIfpxG90_CO"},"outputs":[],"source":["relations = ['replied_to', 'tweeted', 'has_hashtag', 'mentioned', \n","             'created_at', 'announced_on', 'has_keyword', 'in_topic', 'associated_with', \n","             'has_emotion', 'located_at', 'discusses', 'occured_on', 'has_sentiment_toward']\n","\n","relation2ind = {}\n","ind2relation = {}\n","j = 0 \n","for r in relations:\n","  relation2ind[r] = j\n","  ind2relation[j] = r\n","  j += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OOdYKZz04V6u"},"outputs":[],"source":["all_relations={}"]},{"cell_type":"markdown","metadata":{"id":"4w3SlszU-BFS"},"source":["# Create Triples"]},{"cell_type":"markdown","metadata":{"id":"nMW_gNID-Drc"},"source":["## Tweet-to-tweet triples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZoRVQyJ2d231"},"outputs":[],"source":["# parent_df = tweet_df['in_reply_to_status_id']\n","# parent_df = parent_df[~parent_df.isnull()] # clean NaN\n","# parent_df = parent_df.apply(int).apply(str) # make int then str\n","# parent_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hLrCBx9YuNHf"},"outputs":[],"source":["# tweet_dict[1]['user_mentions']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wfl2UA2s96Vg","executionInfo":{"status":"ok","timestamp":1659721704834,"user_tz":420,"elapsed":1011,"user":{"displayName":"James Chen","userId":"10516375287081639459"}},"outputId":"321e5cb4-53f5-4642-9d8b-b532e160c0b8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["6172"]},"metadata":{},"execution_count":17}],"source":["tweet_relations = []\n","\n","for tweet in tweet_dict:\n","  if tweet['in_reply_to_status_id'] != None:\n","    if tweet['in_reply_to_status_id'] in entity2ind['tweet_ids']: # why would tweet['id'] not be in entity2ind?\n","    # print('yuh')                                                  # because the parent tweet might not be relavent to the KG\n","      relation = []\n","      relation.append(entity2ind['tweet_ids'][tweet['id']])\n","      relation.append(entity2ind['tweet_ids'][tweet['in_reply_to_status_id']])\n","      relation.append(relation2ind['replied_to'])\n","      relation.append(1.0)\n","      tweet_relations.append(relation)\n","  # if tweet['quotes']:\n","  #   for quote in tweet['quotes']:\n","  #     if str(quote) in entity2ind:\n","  #       relation = []\n","  #       relation.append(entity2ind[str(tweet['tweet-id'])])\n","  #       relation.append(entity2ind[str(quote)])\n","  #       relation.append(relation2ind['quoted_by'])\n","  #       relation.append(1.0)\n","  #       tweet_relations.append(relation)\n","  #       #print(relation)\n"," \n","  #print(relation)\n","\n","all_relations['tweet->tweet'] = tweet_relations\n","len(tweet_relations)"]},{"cell_type":"markdown","metadata":{"id":"os-fsgVahUN9"},"source":["## Tweet-to-Date"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ufxowuD0hT7O","executionInfo":{"status":"ok","timestamp":1659721707972,"user_tz":420,"elapsed":3147,"user":{"displayName":"James Chen","userId":"10516375287081639459"}},"outputId":"5d14141b-b505-4a53-d080-92f1203d0b21"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 914237/914237 [00:02<00:00, 314171.09it/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["914237"]},"metadata":{},"execution_count":18}],"source":["# date_relations = set()\n","date_relations = []\n","\n","for tweet in tqdm(tweet_dict):\n","  relation = []\n","  relation.append(entity2ind['tweet_ids'][tweet['id']])\n","  relation.append(entity2ind['dates'][tweet['created_at']])\n","  relation.append(relation2ind['created_at']) \n","  relation.append(1.0)\n","  # date_relations.add(tuple(relation))\n","  date_relations.append(tuple(relation))\n","\n","all_relations['tweet->date'] = list(date_relations)\n","len(date_relations)"]},{"cell_type":"markdown","metadata":{"id":"enEFYbn5PD0M"},"source":["Both tweet-to-date and user-to-tweet have 956028 edges, when there are only 923385 tweets and every tweet-to-date edge is supposed to correspond to only one tweet. This means there are repeat tweets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x6ADyrXKQGuq"},"outputs":[],"source":["# len(all_entities['dates'])"]},{"cell_type":"markdown","metadata":{"id":"TstkqNJscvYk"},"source":["## Tweet-to-Hashtags"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qx4_lNj6cx77","executionInfo":{"status":"ok","timestamp":1659721709025,"user_tz":420,"elapsed":1059,"user":{"displayName":"James Chen","userId":"10516375287081639459"}},"outputId":"483ffc4b-c0e3-4c26-e66a-adbb6f0e1dcf"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 914237/914237 [00:01<00:00, 786200.08it/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["253552"]},"metadata":{},"execution_count":20}],"source":["hashtag_relations = []\n","\n","for tweet in tqdm(tweet_dict):\n","  for tag in tweet['hashtags']:\n","    # print(tag)\n","    temp = [] \n","    temp.append(entity2ind['tweet_ids'][tweet['id']])\n","    temp.append(entity2ind['hashtags'][tag['text']])\n","    temp.append(relation2ind['has_hashtag'])\n","    temp.append(1.0)\n","    hashtag_relations.append(temp)\n","    #print(temp)\n","all_relations['tweet->hashtag'] = hashtag_relations\n","len(hashtag_relations)"]},{"cell_type":"markdown","metadata":{"id":"JOzSqoR92MYK"},"source":["## User-to-Tweet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BDbjBciG2TSk","executionInfo":{"status":"ok","timestamp":1659721720521,"user_tz":420,"elapsed":11503,"user":{"displayName":"James Chen","userId":"10516375287081639459"}},"outputId":"af9491d7-e1e5-4484-b19d-7ae61f9d4119"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 914237/914237 [00:11<00:00, 79778.24it/s] \n"]},{"output_type":"execute_result","data":{"text/plain":["914237"]},"metadata":{},"execution_count":21}],"source":["# user_relations = set()\n","user_relations = []\n","\n","for tweet in tqdm(tweet_dict):\n","  relation = []\n","  relation.append(entity2ind['user_ids'][tweet['user']['id']])\n","  relation.append(entity2ind['tweet_ids'][tweet['id']])\n","  relation.append(relation2ind['tweeted']) \n","  relation.append(1.0)\n","  # user_relations.add(tuple(relation))\n","  user_relations.append(relation)\n","all_relations['user->tweet'] = list(user_relations)\n","len(user_relations)"]},{"cell_type":"markdown","metadata":{"id":"rZ2rCym5cD6n"},"source":["## Tweet-to-User"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Uf3JWKIcGbm","executionInfo":{"status":"ok","timestamp":1659721723875,"user_tz":420,"elapsed":3366,"user":{"displayName":"James Chen","userId":"10516375287081639459"}},"outputId":"ea5d81e7-ae7d-4da2-9357-93bfe4170a83"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 914237/914237 [00:03<00:00, 273086.94it/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["1222309"]},"metadata":{},"execution_count":22}],"source":["mention_relations = []\n","for tweet in tqdm(tweet_dict):\n","  for men in tweet['user_mentions']:\n","    temp = []\n","    temp.append(entity2ind['tweet_ids'][tweet['id']])\n","    temp.append(entity2ind['user_ids'][men['id']])\n","    temp.append(relation2ind['mentioned'])\n","    temp.append(1.0)\n","    mention_relations.append(temp)\n","    #print(temp)\n","all_relations['tweet->user'] = mention_relations\n","len(mention_relations)"]},{"cell_type":"markdown","metadata":{"id":"qWXWv5Sz4oVL"},"source":["## Tweet to Geo"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OA3xbAiC4nZd","executionInfo":{"status":"ok","timestamp":1659721724188,"user_tz":420,"elapsed":319,"user":{"displayName":"James Chen","userId":"10516375287081639459"}},"outputId":"e2b1229e-f8f5-474e-863c-8bdd1ca5a7c5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["733"]},"metadata":{},"execution_count":23}],"source":["geo_relations = []\n","\n","for tweet in tweet_dict:\n","  location = tweet['geo']\n","  if location != None:\n","    relation = []\n","    relation.append(entity2ind['tweet_ids'][tweet['id']])\n","    relation.append(entity2ind['geo'][tuple(location['coordinates'])])\n","    relation.append(relation2ind['located_at']) \n","    relation.append(1.0)\n","    geo_relations.append(relation)\n","    #print(relation)\n","all_relations['tweet->geo'] = geo_relations\n","len(geo_relations)"]},{"cell_type":"markdown","metadata":{"id":"lYeOLqLT1gOp"},"source":["## Tweet to Topic"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2P6D1HFSp4hK","executionInfo":{"status":"ok","timestamp":1659721725170,"user_tz":420,"elapsed":993,"user":{"displayName":"James Chen","userId":"10516375287081639459"}},"outputId":"5f104e58-8fd6-4cbe-ad9f-8aa99d264584"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 923385/923385 [00:01<00:00, 913621.89it/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["291264"]},"metadata":{},"execution_count":24}],"source":["topic_relations = []\n","for tweet in tqdm(topic_tweets_dict):\n","  if tweet['topic'] != -1:\n","    temp = []\n","    temp.append(entity2ind['tweet_ids'][tweet['id']])\n","    temp.append(tweet['topic'])                       # topics are enumerated naturally\n","    temp.append(relation2ind['in_topic'])\n","    temp.append(1.0)\n","    topic_relations.append(temp)\n","    #print(temp)\n","all_relations['tweet->topic'] = topic_relations\n","len(topic_relations)"]},{"cell_type":"markdown","metadata":{"id":"3UcffezdUtHK"},"source":["## Tweet-to-Emotion"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Rf_Onh-v8KG","executionInfo":{"status":"ok","timestamp":1659721727511,"user_tz":420,"elapsed":2356,"user":{"displayName":"James Chen","userId":"10516375287081639459"}},"outputId":"c1fde548-0fc5-4c5a-ef0e-557e97cb5480"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 923385/923385 [00:02<00:00, 408471.52it/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["784205"]},"metadata":{},"execution_count":25}],"source":["emotion_relations = []\n","for tweet in tqdm(emotion_tweets_dict):\n","  if tweet['cleaned_text'] != '[]' and tweet['emotion'] in all_entities['emotions']:\n","    temp = []\n","    temp.append(entity2ind['tweet_ids'][tweet['id']])\n","    temp.append(entity2ind['emotions'][tweet['emotion']])\n","    temp.append(relation2ind['has_emotion'])\n","    temp.append(1.0)\n","    emotion_relations.append(temp)\n","    #print(temp)\n","all_relations['tweet->emotion'] = emotion_relations\n","len(emotion_relations)"]},{"cell_type":"markdown","metadata":{"id":"6RcIXKapbr0U"},"source":["## Tweet-to-Semantic Entity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JeTLAi6zbreE"},"outputs":[],"source":["# sem_ent_relations = []\n","# for tweet in sem_ent_dict:\n","#   for ent in sem_ent_dict[tweet]:\n","#     temp = []\n","#     temp.append(entity2ind['tweet_ids'][tweet])\n","#     temp.append(entity2ind['sem_ents'][ent])\n","#     temp.append(relation2ind['discusses'])\n","#     temp.append(1.0)\n","#     sem_ent_relations.append(temp)\n","#     #print(temp)\n","# all_relations['tweet->sem_ent'] = sem_ent_relations\n","# len(sem_ent_relations)"]},{"cell_type":"markdown","metadata":{"id":"lLooMAFG14W-"},"source":["## Event-to-Date"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NozPvBkN12n5","executionInfo":{"status":"ok","timestamp":1659721727513,"user_tz":420,"elapsed":16,"user":{"displayName":"James Chen","userId":"10516375287081639459"}},"outputId":"9e7121bf-a055-4b6a-f21c-2a6cda230afa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["24"]},"metadata":{},"execution_count":27}],"source":["event_relations = []\n","\n","for datetime, event in date_to_event_dict.items():\n","  relation = []\n","  relation.append(entity2ind['events'][event])\n","  relation.append(entity2ind['dates'][datetime])\n","  relation.append(relation2ind['occured_on']) \n","  relation.append(1.0)\n","  event_relations.append(relation)\n","\n","all_relations['event->date'] = event_relations\n","len(event_relations)"]},{"cell_type":"markdown","metadata":{"id":"M10muid1oDFc"},"source":["## User to Topic via Sentiment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"URDzuV-6oCeu"},"outputs":[],"source":["sent_df = pd.read_csv(path+'df_tweets_sentiment_scores.csv')\n","sent_dict = sent_df.to_dict(orient='records')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g3ZV8KemTJfC"},"outputs":[],"source":["topic_dict = dict()\n","topic_weights = dict()\n","for tweet in topic_tweets_dict:\n","  topic_dict[tweet['id']] = tweet['topic']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CSoqXj-ModqB","executionInfo":{"status":"ok","timestamp":1659721759865,"user_tz":420,"elapsed":15653,"user":{"displayName":"James Chen","userId":"10516375287081639459"}},"outputId":"b705c741-621e-4125-d81d-e7c87123048f"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 923385/923385 [00:15<00:00, 59021.46it/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["285178"]},"metadata":{},"execution_count":30}],"source":["import ast\n","sent_relations = []\n","discovery = []\n","\n","for tweet in tqdm(sent_dict):\n","  if tweet['user_id'] not in bot_list and topic_dict[tweet['id']] != -1:\n","    relation = []                                                           #only connect users to topics that \n","    relation.append(entity2ind['user_ids'][tweet['user_id']])\n","    relation.append(entity2ind['topics'][topic_dict[tweet['id']]])\n","    relation.append(relation2ind['has_sentiment_toward']) \n","    relation.append(tweet['sentiment_score'])\n","    # discovery.append(topic_weights[tweet['id']])\n","    sent_relations.append(relation)\n","\n","all_relations['user->topic'] = sent_relations\n","len(sent_relations)"]},{"cell_type":"code","source":["# import matplotlib.pyplot as plt\n","\n","# quartiles = np.percentile(discovery, [25, 50, 75])\n","# plt.boxplot(discovery)\n","# quartiles"],"metadata":{"id":"Q1SyW3KXl8AV","executionInfo":{"status":"error","timestamp":1659721760284,"user_tz":420,"elapsed":446,"user":{"displayName":"James Chen","userId":"10516375287081639459"}},"colab":{"base_uri":"https://localhost:8080/","height":415},"outputId":"78ac1912-fb8a-4899-dd5d-a0f1af326d49"},"execution_count":null,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-7d7302e2e932>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mquartiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscovery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m75\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscovery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mquartiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mpercentile\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mpercentile\u001b[0;34m(a, q, axis, out, overwrite_input, interpolation, keepdims)\u001b[0m\n\u001b[1;32m   3866\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Percentiles must be in the range [0, 100]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3867\u001b[0m     return _quantile_unchecked(\n\u001b[0;32m-> 3868\u001b[0;31m         a, q, axis, out, overwrite_input, interpolation, keepdims)\n\u001b[0m\u001b[1;32m   3869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_quantile_unchecked\u001b[0;34m(a, q, axis, out, overwrite_input, interpolation, keepdims)\u001b[0m\n\u001b[1;32m   3986\u001b[0m     r, k = _ureduce(a, func=_quantile_ureduce_func, q=q, axis=axis, out=out,\n\u001b[1;32m   3987\u001b[0m                     \u001b[0moverwrite_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverwrite_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3988\u001b[0;31m                     interpolation=interpolation)\n\u001b[0m\u001b[1;32m   3989\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3990\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3562\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3564\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3565\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_quantile_ureduce_func\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4096\u001b[0m                 \u001b[0mindices_below\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_above\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4097\u001b[0m             )), axis=0)\n\u001b[0;32m-> 4098\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4099\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;31m# cannot contain nan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index -1 is out of bounds for axis 0 with size 0"]}]},{"cell_type":"code","source":["print(sum([len(all_relations[key]) for key in all_relations]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UP2OuT75MajH","executionInfo":{"status":"ok","timestamp":1659722927990,"user_tz":420,"elapsed":132,"user":{"displayName":"James Chen","userId":"10516375287081639459"}},"outputId":"24cd1824-06b8-4bcc-f67b-63a7aaeaa970"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4671911\n"]}]},{"cell_type":"markdown","metadata":{"id":"j2_Lr52R34nh"},"source":["## Pickle time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X3Rb7yFIWMwn"},"outputs":[],"source":["path = \"drive/MyDrive/Knowledge Graphs 2022/KG construction/\"\n","with open(path+'all_entities.pickle', 'wb') as file:\n","    pickle.dump(all_entities, file, protocol=pickle.HIGHEST_PROTOCOL)\n","with open(path+'all_relations.pickle', 'wb') as file:\n","    pickle.dump(all_relations, file, protocol=pickle.HIGHEST_PROTOCOL)"]}],"metadata":{"colab":{"collapsed_sections":[],"machine_shape":"hm","name":"KG Construction.ipynb","provenance":[{"file_id":"1muvEpiZx-g2-baI68kmJ1tTZ4Hyt5maR","timestamp":1655672760181}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}